{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.debugger import Rule\n",
    "from sagemaker.debugger import rule_configs\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.session import TrainingInput\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Bucket: sagemaker-us-east-1-013268419919\n",
      "AWS Region: us-east-1\n",
      "RoleArn: arn:aws:iam::013268419919:role/service-role/AmazonSageMaker-ExecutionRole-20210829T170717\n"
     ]
    }
   ],
   "source": [
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f'Default Bucket: {bucket}')\n",
    "print(f'AWS Region: {region}')\n",
    "print(f'RoleArn: {role}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DEFAULT_S3_BUCKET\"] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = f\"s3://{bucket}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-13 12:54:08--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 169001437 (161M) [application/x-gzip]\n",
      "Saving to: ‘cifar-100-python.tar.gz’\n",
      "\n",
      "cifar-100-python.ta 100%[===================>] 161.17M  84.0MB/s    in 1.9s    \n",
      "\n",
      "2021-09-13 12:54:11 (84.0 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-100-python/\n",
      "cifar-100-python/file.txt~\n",
      "tar: cifar-100-python/file.txt~: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "cifar-100-python/train\n",
      "tar: cifar-100-python/train: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "cifar-100-python/test\n",
      "tar: cifar-100-python/test: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "cifar-100-python/meta\n",
      "tar: cifar-100-python/meta: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: cifar-100-python: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "! tar -xvf cifar-100-python.tar.gz && rm cifar-100-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_dataset(path: str):\n",
    "    \"\"\"\n",
    "    Unpickles the dataset at given path\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta = unload_dataset(\"./cifar-100-python/meta\")\n",
    "dataset_train = unload_dataset(\"./cifar-100-python/train\")\n",
    "dataset_test = unload_dataset(\"./cifar-100-python/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration, Transformation & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_train[b'data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({\n",
    "    \"filenames\": dataset_train[b'filenames'],\n",
    "    \"labels\": dataset_train[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_train[b'filenames']))\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"filenames\": dataset_test[b'filenames'],\n",
    "    \"labels\": dataset_test[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_test[b'filenames']))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: pd.DataFrame, allowed_labels: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    * Removes rows where label != 8 or 48\n",
    "    * Decodes filenames to regular str\n",
    "    \"\"\"\n",
    "    allowed_labels = allowed_labels or [\"8\", \"48\"]\n",
    "    temp = data.loc[data['labels'].isin(allowed_labels)]\n",
    "    idx = list(temp.index)\n",
    "    temp = temp.reset_index()\n",
    "    temp[\"filenames\"] = temp[\"filenames\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "    return temp, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(dataset: dict, data: np.ndarray, output_dir=str, verbose: bool=False) -> None:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for filename, image in zip(dataset['filenames'], data):\n",
    "        channels = [\n",
    "            image[1024*x:1024*(x+1)] for x in range(3)\n",
    "        ]\n",
    "        combined = np.dstack([channel.reshape(32, 32) for channel in channels])\n",
    "        plt.imsave(os.path.join(output_dir, filename), combined)\n",
    "        if verbose:\n",
    "            print(f\"Saved {filename} to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean, train_idx = clean_data(df_train)\n",
    "df_test_clean, test_idx = clean_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(\n",
    "    df_train_clean, dataset_train[b'data'], output_dir=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(\n",
    "    df_test_clean, dataset_test[b'data'], output_dir=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 sync ./train s3://${DEFAULT_S3_BUCKET}/train/ > train.log 2>&1\n",
    "! aws s3 sync ./test s3://${DEFAULT_S3_BUCKET}/test/ > test.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_file(df: pd.DataFrame, prefix: str) -> None:\n",
    "    df[\"s3_path\"] = df[\"filenames\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply(lambda x: 0 if x==8 else 1)\n",
    "    df[[\"row\", \"labels\", \"s3_path\"]].to_csv(\n",
    "        f\"{prefix}.lst\", sep=\"\\t\", index=False, header=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metadata_file(df_train_clean, \"train\")\n",
    "create_metadata_file(df_test_clean, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "(boto3.Session()\n",
    " .resource(\"s3\")\n",
    " .Bucket(bucket)\n",
    " .Object(\"train.lst\")\n",
    " .upload_file(\"./train.lst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "(boto3.Session()\n",
    " .resource(\"s3\")\n",
    " .Bucket(bucket)\n",
    " .Object(\"test.lst\")\n",
    " .upload_file(\"./test.lst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_image = sagemaker.image_uris.retrieve(\n",
    "    framework=\"image-classification\", region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = BASE_URL + \"/models/image_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#continuous-analyses-through-rules\n",
    "debugger_rules = [\n",
    "    rule_configs.vanishing_gradient(),\n",
    "    rule_configs.weight_update_ratio(),\n",
    "    rule_configs.loss_not_decreasing(),\n",
    "    rule_configs.overtraining()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Estimator(\n",
    "    image_uri=algo_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    "    rules=[\n",
    "        Rule.sagemaker(rule) for rule in debugger_rules\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_hyperparameters(\n",
    "    image_shape=\"3,32,32\",\n",
    "    num_classes=2,\n",
    "    num_training_samples=len(df_train_clean),\n",
    "    use_pretrained_model=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = {\n",
    "    \"train\": TrainingInput(\n",
    "        s3_data=f\"{BASE_URL}/train/\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"validation\": TrainingInput(\n",
    "        s3_data=f\"{BASE_URL}/test/\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"train_lst\": TrainingInput(\n",
    "        s3_data=f\"{BASE_URL}/train.lst\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"validation_lst\": TrainingInput(\n",
    "        s3_data=f\"{BASE_URL}/test.lst\",\n",
    "        content_type=\"application/x-image\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-13 16:22:28 Starting - Starting the training job...\n",
      "2021-09-13 16:22:52 Starting - Launching requested ML instancesVanishingGradient: InProgress\n",
      "WeightUpdateRatio: InProgress\n",
      "LossNotDecreasing: InProgress\n",
      "Overtraining: InProgress\n",
      "ProfilerReport-1631550148: InProgress\n",
      "......\n",
      "2021-09-13 16:23:53 Starting - Preparing the instances for training............\n",
      "2021-09-13 16:26:00 Downloading - Downloading input data...\n",
      "2021-09-13 16:26:20 Training - Downloading the training image...\n",
      "2021-09-13 16:27:00 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'num_classes': '2', 'num_training_samples': '1000', 'use_pretrained_model': '1', 'image_shape': '3,32,32'}\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Final configuration: {'use_pretrained_model': '1', 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,32,32', 'precision_dtype': 'float32', 'num_classes': '2', 'num_training_samples': '1000'}\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Searching for .lst files in /opt/ml/input/data/train_lst.\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Creating record files for train.lst\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Done creating record files...\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Searching for .lst files in /opt/ml/input/data/validation_lst.\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Creating record files for test.lst\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Done creating record files...\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] multi_label: 0\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] num_layers: 152\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] data type: <class 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] epochs: 30\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] learning_rate: 0.1\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] num_training_samples: 1000\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] mini_batch_size: 32\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] image_shape: 3,32,32\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] num_classes: 2\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] kv_store: device\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:55 INFO 139645297174336] --------------------\u001b[0m\n",
      "\u001b[34m[16:26:55] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.6753.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[16:26:55] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.6753.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:26:57 INFO 139645297174336] Setting number of threads: 3\u001b[0m\n",
      "\u001b[34m[16:27:11] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.6753.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:22 INFO 139645297174336] Epoch[0] Batch [20]#011Speed: 54.418 samples/sec#011accuracy=0.511905\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:24 INFO 139645297174336] Epoch[0] Train-accuracy=0.507056\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:24 INFO 139645297174336] Epoch[0] Time cost=13.781\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:24 INFO 139645297174336] Epoch[0] Validation-accuracy=0.484375\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:25 INFO 139645297174336] Storing the best model with validation accuracy: 0.484375\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:25 INFO 139645297174336] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:30 INFO 139645297174336] Epoch[1] Batch [20]#011Speed: 147.485 samples/sec#011accuracy=0.516369\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:32 INFO 139645297174336] Epoch[1] Train-accuracy=0.522177\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:32 INFO 139645297174336] Epoch[1] Time cost=6.386\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:32 INFO 139645297174336] Epoch[1] Validation-accuracy=0.536458\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:33 INFO 139645297174336] Storing the best model with validation accuracy: 0.536458\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:33 INFO 139645297174336] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:37 INFO 139645297174336] Epoch[2] Batch [20]#011Speed: 154.728 samples/sec#011accuracy=0.501488\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:39 INFO 139645297174336] Epoch[2] Train-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:39 INFO 139645297174336] Epoch[2] Time cost=6.147\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:40 INFO 139645297174336] Epoch[2] Validation-accuracy=0.510417\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:45 INFO 139645297174336] Epoch[3] Batch [20]#011Speed: 155.090 samples/sec#011accuracy=0.508929\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:47 INFO 139645297174336] Epoch[3] Train-accuracy=0.504032\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:47 INFO 139645297174336] Epoch[3] Time cost=6.169\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:47 INFO 139645297174336] Epoch[3] Validation-accuracy=0.531250\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:52 INFO 139645297174336] Epoch[4] Batch [20]#011Speed: 153.292 samples/sec#011accuracy=0.485119\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:54 INFO 139645297174336] Epoch[4] Train-accuracy=0.467742\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:54 INFO 139645297174336] Epoch[4] Time cost=6.196\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:54 INFO 139645297174336] Epoch[4] Validation-accuracy=0.484375\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:27:59 INFO 139645297174336] Epoch[5] Batch [20]#011Speed: 155.171 samples/sec#011accuracy=0.553571\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:01 INFO 139645297174336] Epoch[5] Train-accuracy=0.559476\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:01 INFO 139645297174336] Epoch[5] Time cost=6.131\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:02 INFO 139645297174336] Epoch[5] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:07 INFO 139645297174336] Epoch[6] Batch [20]#011Speed: 139.660 samples/sec#011accuracy=0.519345\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:09 INFO 139645297174336] Epoch[6] Train-accuracy=0.496976\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:09 INFO 139645297174336] Epoch[6] Time cost=6.590\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:10 INFO 139645297174336] Epoch[6] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:14 INFO 139645297174336] Epoch[7] Batch [20]#011Speed: 154.289 samples/sec#011accuracy=0.516369\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:16 INFO 139645297174336] Epoch[7] Train-accuracy=0.510081\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:16 INFO 139645297174336] Epoch[7] Time cost=6.159\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:17 INFO 139645297174336] Epoch[7] Validation-accuracy=0.433036\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:22 INFO 139645297174336] Epoch[8] Batch [20]#011Speed: 154.753 samples/sec#011accuracy=0.507440\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:24 INFO 139645297174336] Epoch[8] Train-accuracy=0.529234\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:24 INFO 139645297174336] Epoch[8] Time cost=6.168\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:24 INFO 139645297174336] Epoch[8] Validation-accuracy=0.494792\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:29 INFO 139645297174336] Epoch[9] Batch [20]#011Speed: 155.323 samples/sec#011accuracy=0.480655\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:31 INFO 139645297174336] Epoch[9] Train-accuracy=0.487903\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:31 INFO 139645297174336] Epoch[9] Time cost=6.124\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:32 INFO 139645297174336] Epoch[9] Validation-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:36 INFO 139645297174336] Epoch[10] Batch [20]#011Speed: 152.060 samples/sec#011accuracy=0.504464\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:38 INFO 139645297174336] Epoch[10] Train-accuracy=0.504032\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:38 INFO 139645297174336] Epoch[10] Time cost=6.247\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:39 INFO 139645297174336] Epoch[10] Validation-accuracy=0.479167\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:44 INFO 139645297174336] Epoch[11] Batch [20]#011Speed: 153.931 samples/sec#011accuracy=0.501488\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:46 INFO 139645297174336] Epoch[11] Train-accuracy=0.505040\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:46 INFO 139645297174336] Epoch[11] Time cost=6.157\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:46 INFO 139645297174336] Epoch[11] Validation-accuracy=0.508929\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:51 INFO 139645297174336] Epoch[12] Batch [20]#011Speed: 152.716 samples/sec#011accuracy=0.458333\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:53 INFO 139645297174336] Epoch[12] Train-accuracy=0.472782\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:53 INFO 139645297174336] Epoch[12] Time cost=6.217\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:54 INFO 139645297174336] Epoch[12] Validation-accuracy=0.453125\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:28:58 INFO 139645297174336] Epoch[13] Batch [20]#011Speed: 155.680 samples/sec#011accuracy=0.471726\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:00 INFO 139645297174336] Epoch[13] Train-accuracy=0.475806\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:00 INFO 139645297174336] Epoch[13] Time cost=6.171\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:01 INFO 139645297174336] Epoch[13] Validation-accuracy=0.520833\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:06 INFO 139645297174336] Epoch[14] Batch [20]#011Speed: 136.603 samples/sec#011accuracy=0.489583\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:08 INFO 139645297174336] Epoch[14] Train-accuracy=0.485887\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:08 INFO 139645297174336] Epoch[14] Time cost=6.735\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:09 INFO 139645297174336] Epoch[14] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:14 INFO 139645297174336] Epoch[15] Batch [20]#011Speed: 156.558 samples/sec#011accuracy=0.526786\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:16 INFO 139645297174336] Epoch[15] Train-accuracy=0.516129\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:16 INFO 139645297174336] Epoch[15] Time cost=6.150\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:16 INFO 139645297174336] Epoch[15] Validation-accuracy=0.486607\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:21 INFO 139645297174336] Epoch[16] Batch [20]#011Speed: 152.065 samples/sec#011accuracy=0.505952\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:23 INFO 139645297174336] Epoch[16] Train-accuracy=0.523185\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:23 INFO 139645297174336] Epoch[16] Time cost=6.210\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:24 INFO 139645297174336] Epoch[16] Validation-accuracy=0.494792\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:28 INFO 139645297174336] Epoch[17] Batch [20]#011Speed: 154.886 samples/sec#011accuracy=0.523810\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:31 INFO 139645297174336] Epoch[17] Train-accuracy=0.506048\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:31 INFO 139645297174336] Epoch[17] Time cost=6.191\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:31 INFO 139645297174336] Epoch[17] Validation-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:36 INFO 139645297174336] Epoch[18] Batch [20]#011Speed: 152.817 samples/sec#011accuracy=0.468750\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:38 INFO 139645297174336] Epoch[18] Train-accuracy=0.470766\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:38 INFO 139645297174336] Epoch[18] Time cost=6.205\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:38 INFO 139645297174336] Epoch[18] Validation-accuracy=0.453125\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:43 INFO 139645297174336] Epoch[19] Batch [20]#011Speed: 153.928 samples/sec#011accuracy=0.525298\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:45 INFO 139645297174336] Epoch[19] Train-accuracy=0.518145\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:45 INFO 139645297174336] Epoch[19] Time cost=6.167\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:46 INFO 139645297174336] Epoch[19] Validation-accuracy=0.491071\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:50 INFO 139645297174336] Epoch[20] Batch [20]#011Speed: 154.010 samples/sec#011accuracy=0.505952\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:52 INFO 139645297174336] Epoch[20] Train-accuracy=0.505040\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:52 INFO 139645297174336] Epoch[20] Time cost=6.161\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:53 INFO 139645297174336] Epoch[20] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:29:58 INFO 139645297174336] Epoch[21] Batch [20]#011Speed: 155.954 samples/sec#011accuracy=0.495536\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:00 INFO 139645297174336] Epoch[21] Train-accuracy=0.489919\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:00 INFO 139645297174336] Epoch[21] Time cost=6.124\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:00 INFO 139645297174336] Epoch[21] Validation-accuracy=0.473958\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:05 INFO 139645297174336] Epoch[22] Batch [20]#011Speed: 145.883 samples/sec#011accuracy=0.507440\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:08 INFO 139645297174336] Epoch[22] Train-accuracy=0.501008\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:08 INFO 139645297174336] Epoch[22] Time cost=6.654\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:08 INFO 139645297174336] Epoch[22] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:13 INFO 139645297174336] Epoch[23] Batch [20]#011Speed: 154.662 samples/sec#011accuracy=0.507440\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:15 INFO 139645297174336] Epoch[23] Train-accuracy=0.520161\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:15 INFO 139645297174336] Epoch[23] Time cost=6.136\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:15 INFO 139645297174336] Epoch[23] Validation-accuracy=0.455357\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:20 INFO 139645297174336] Epoch[24] Batch [20]#011Speed: 153.640 samples/sec#011accuracy=0.532738\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:22 INFO 139645297174336] Epoch[24] Train-accuracy=0.531250\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:22 INFO 139645297174336] Epoch[24] Time cost=6.182\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:23 INFO 139645297174336] Epoch[24] Validation-accuracy=0.453125\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:28 INFO 139645297174336] Epoch[25] Batch [20]#011Speed: 152.800 samples/sec#011accuracy=0.511905\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:30 INFO 139645297174336] Epoch[25] Train-accuracy=0.535282\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:30 INFO 139645297174336] Epoch[25] Time cost=6.195\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:30 INFO 139645297174336] Epoch[25] Validation-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:35 INFO 139645297174336] Epoch[26] Batch [20]#011Speed: 154.158 samples/sec#011accuracy=0.485119\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:37 INFO 139645297174336] Epoch[26] Train-accuracy=0.504032\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:37 INFO 139645297174336] Epoch[26] Time cost=6.222\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:37 INFO 139645297174336] Epoch[26] Validation-accuracy=0.447917\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:42 INFO 139645297174336] Epoch[27] Batch [20]#011Speed: 156.227 samples/sec#011accuracy=0.513393\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:44 INFO 139645297174336] Epoch[27] Train-accuracy=0.517137\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:44 INFO 139645297174336] Epoch[27] Time cost=6.096\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:45 INFO 139645297174336] Epoch[27] Validation-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:49 INFO 139645297174336] Epoch[28] Batch [20]#011Speed: 153.936 samples/sec#011accuracy=0.519345\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:51 INFO 139645297174336] Epoch[28] Train-accuracy=0.525202\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:51 INFO 139645297174336] Epoch[28] Time cost=6.167\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:52 INFO 139645297174336] Epoch[28] Validation-accuracy=0.494792\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:57 INFO 139645297174336] Epoch[29] Batch [20]#011Speed: 155.715 samples/sec#011accuracy=0.514881\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:59 INFO 139645297174336] Epoch[29] Train-accuracy=0.531250\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:59 INFO 139645297174336] Epoch[29] Time cost=6.119\u001b[0m\n",
      "\u001b[34m[09/13/2021 16:30:59 INFO 139645297174336] Epoch[29] Validation-accuracy=0.494792\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.fit(inputs=model_inputs, logs=\"All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"awako-endpoint-\"+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s2_uri=f\"{BASE_URL}/data_capture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = model.deploy(\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    endpoint_name=\"awako-endpoint\"\n",
    "    data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = RealTimePredictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    content_type=\"image/png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = IdentitySerializer()\"image/png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./test/bicycle_s_001789.png\", \"rb\") as f:\n",
    "    payload = f.read()\n",
    "    \n",
    "response = predictor.predict(data=payload)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
