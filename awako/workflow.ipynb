{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh: 0: getcwd() failed: No such file or directory\n",
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "! pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from time import gmtime\n",
    "from time import strftime\n",
    "\n",
    "import boto3\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.debugger import Rule\n",
    "from sagemaker.debugger import rule_configs\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.session import TrainingInput\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Bucket: sagemaker-us-east-1-013268419919\n",
      "AWS Region: us-east-1\n",
      "RoleArn: arn:aws:iam::013268419919:role/service-role/AmazonSageMaker-ExecutionRole-20210829T170717\n"
     ]
    }
   ],
   "source": [
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f'Default Bucket: {bucket}')\n",
    "print(f'AWS Region: {region}')\n",
    "print(f'RoleArn: {role}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DEFAULT_S3_BUCKET\"] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = f\"s3://{bucket}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-13 12:54:08--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 169001437 (161M) [application/x-gzip]\n",
      "Saving to: ‘cifar-100-python.tar.gz’\n",
      "\n",
      "cifar-100-python.ta 100%[===================>] 161.17M  84.0MB/s    in 1.9s    \n",
      "\n",
      "2021-09-13 12:54:11 (84.0 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar-100-python/\n",
      "cifar-100-python/file.txt~\n",
      "tar: cifar-100-python/file.txt~: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "cifar-100-python/train\n",
      "tar: cifar-100-python/train: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "cifar-100-python/test\n",
      "tar: cifar-100-python/test: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "cifar-100-python/meta\n",
      "tar: cifar-100-python/meta: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: cifar-100-python: Cannot change ownership to uid 1000, gid 1000: Operation not permitted\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "! tar -xvf cifar-100-python.tar.gz && rm cifar-100-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_dataset(path: str):\n",
    "    \"\"\"\n",
    "    Unpickles the dataset at given path\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta = unload_dataset(\"./cifar-100-python/meta\")\n",
    "dataset_train = unload_dataset(\"./cifar-100-python/train\")\n",
    "dataset_test = unload_dataset(\"./cifar-100-python/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration, Transformation & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_train[b'data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({\n",
    "    \"filenames\": dataset_train[b'filenames'],\n",
    "    \"labels\": dataset_train[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_train[b'filenames']))\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"filenames\": dataset_test[b'filenames'],\n",
    "    \"labels\": dataset_test[b'fine_labels'],\n",
    "    \"row\": range(len(dataset_test[b'filenames']))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: pd.DataFrame, allowed_labels: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    * Removes rows where label != 8 or 48\n",
    "    * Decodes filenames to regular str\n",
    "    \"\"\"\n",
    "    allowed_labels = allowed_labels or [\"8\", \"48\"]\n",
    "    temp = data.loc[data['labels'].isin(allowed_labels)]\n",
    "    idx = list(temp.index)\n",
    "    temp = temp.reset_index()\n",
    "    temp[\"filenames\"] = temp[\"filenames\"].apply(lambda x: x.decode(\"utf-8\"))\n",
    "    return temp, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(dataset: dict, data: np.ndarray, output_dir=str, verbose: bool=False) -> None:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for filename, image in zip(dataset['filenames'], data):\n",
    "        channels = [\n",
    "            image[1024*x:1024*(x+1)] for x in range(3)\n",
    "        ]\n",
    "        combined = np.dstack([channel.reshape(32, 32) for channel in channels])\n",
    "        plt.imsave(os.path.join(output_dir, filename), combined)\n",
    "        if verbose:\n",
    "            print(f\"Saved {filename} to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean, train_idx = clean_data(df_train)\n",
    "df_test_clean, test_idx = clean_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(\n",
    "    df_train_clean, dataset_train[b'data'], output_dir=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(\n",
    "    df_test_clean, dataset_test[b'data'], output_dir=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 sync ./train s3://${DEFAULT_S3_BUCKET}/train/ > train.log 2>&1\n",
    "! aws s3 sync ./test s3://${DEFAULT_S3_BUCKET}/test/ > test.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_file(df: pd.DataFrame, prefix: str) -> None:\n",
    "    df[\"s3_path\"] = df[\"filenames\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply(lambda x: 0 if x==8 else 1)\n",
    "    df[[\"row\", \"labels\", \"s3_path\"]].to_csv(\n",
    "        f\"{prefix}.lst\", sep=\"\\t\", index=False, header=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metadata_file(df_train_clean, \"train\")\n",
    "create_metadata_file(df_test_clean, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "(boto3.Session()\n",
    " .resource(\"s3\")\n",
    " .Bucket(bucket)\n",
    " .Object(\"train.lst\")\n",
    " .upload_file(\"./train.lst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "(boto3.Session()\n",
    " .resource(\"s3\")\n",
    " .Bucket(bucket)\n",
    " .Object(\"test.lst\")\n",
    " .upload_file(\"./test.lst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_image = sagemaker.image_uris.retrieve(\n",
    "    framework=\"image-classification\", region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = BASE_URL + \"/models/image_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#continuous-analyses-through-rules\n",
    "debugger_rules = [\n",
    "    rule_configs.vanishing_gradient(),\n",
    "    rule_configs.weight_update_ratio(),\n",
    "    rule_configs.loss_not_decreasing(),\n",
    "    rule_configs.overtraining()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Estimator(\n",
    "    image_uri=algo_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    "    rules=[\n",
    "        Rule.sagemaker(rule) for rule in debugger_rules\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_hyperparameters(\n",
    "    image_shape=\"3,32,32\",\n",
    "    num_classes=2,\n",
    "    num_training_samples=len(df_train_clean),\n",
    "    use_pretrained_model=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = {\n",
    "    \"train\": TrainingInput(\n",
    "        s3_data=f\"{BASE_URL}/train/\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"validation\": TrainingInput(\n",
    "        s3_data=f\"{BASE_URL}/test/\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"train_lst\": TrainingInput(\n",
    "        s3_data=f\"{BASE_URL}/train.lst\",\n",
    "        content_type=\"application/x-image\"\n",
    "    ),\n",
    "    \"validation_lst\": TrainingInput(\n",
    "        s3_data=f\"{BASE_URL}/test.lst\",\n",
    "        content_type=\"application/x-image\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-13 18:32:07 Starting - Starting the training job...\n",
      "2021-09-13 18:32:13 Starting - Launching requested ML instancesVanishingGradient: InProgress\n",
      "WeightUpdateRatio: InProgress\n",
      "LossNotDecreasing: InProgress\n",
      "Overtraining: InProgress\n",
      "ProfilerReport-1631557927: InProgress\n",
      ".........\n",
      "2021-09-13 18:34:01 Starting - Preparing the instances for training.........\n",
      "2021-09-13 18:35:28 Downloading - Downloading input data......\n",
      "2021-09-13 18:36:22 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'num_classes': '2', 'num_training_samples': '1000', 'use_pretrained_model': '0', 'image_shape': '3,32,32'}\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Final configuration: {'use_pretrained_model': '0', 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,32,32', 'precision_dtype': 'float32', 'num_classes': '2', 'num_training_samples': '1000'}\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Searching for .lst files in /opt/ml/input/data/train_lst.\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Creating record files for train.lst\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Done creating record files...\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Searching for .lst files in /opt/ml/input/data/validation_lst.\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Creating record files for test.lst\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Done creating record files...\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] use_pretrained_model: 0\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] multi_label: 0\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Performing random weight initialization\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] num_layers: 152\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] data type: <class 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] epochs: 30\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] learning_rate: 0.1\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] num_training_samples: 1000\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] mini_batch_size: 32\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] image_shape: 3,32,32\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] num_classes: 2\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] kv_store: device\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] --------------------\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:36:49 INFO 140489187047232] Setting number of threads: 3\u001b[0m\n",
      "\n",
      "2021-09-13 18:37:03 Training - Training image download completed. Training in progress.\u001b[34m[18:36:57] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.6753.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:17 INFO 140489187047232] Epoch[0] Batch [20]#011Speed: 32.827 samples/sec#011accuracy=0.541667\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:22 INFO 140489187047232] Epoch[0] Train-accuracy=0.525202\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:22 INFO 140489187047232] Epoch[0] Time cost=25.350\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:24 INFO 140489187047232] Epoch[0] Validation-accuracy=0.494792\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:24 INFO 140489187047232] Storing the best model with validation accuracy: 0.494792\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:25 INFO 140489187047232] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:37 INFO 140489187047232] Epoch[1] Batch [20]#011Speed: 53.510 samples/sec#011accuracy=0.489583\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:43 INFO 140489187047232] Epoch[1] Train-accuracy=0.492944\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:43 INFO 140489187047232] Epoch[1] Time cost=17.853\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:44 INFO 140489187047232] Epoch[1] Validation-accuracy=0.484375\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:37:57 INFO 140489187047232] Epoch[2] Batch [20]#011Speed: 53.412 samples/sec#011accuracy=0.514881\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:03 INFO 140489187047232] Epoch[2] Train-accuracy=0.507056\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:03 INFO 140489187047232] Epoch[2] Time cost=18.009\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:04 INFO 140489187047232] Epoch[2] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:05 INFO 140489187047232] Storing the best model with validation accuracy: 0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:05 INFO 140489187047232] Saved checkpoint to \"/opt/ml/model/image-classification-0003.params\"\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:17 INFO 140489187047232] Epoch[3] Batch [20]#011Speed: 53.186 samples/sec#011accuracy=0.479167\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:23 INFO 140489187047232] Epoch[3] Train-accuracy=0.486895\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:23 INFO 140489187047232] Epoch[3] Time cost=17.959\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:25 INFO 140489187047232] Epoch[3] Validation-accuracy=0.491071\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:38 INFO 140489187047232] Epoch[4] Batch [20]#011Speed: 52.991 samples/sec#011accuracy=0.510417\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:44 INFO 140489187047232] Epoch[4] Train-accuracy=0.510081\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:44 INFO 140489187047232] Epoch[4] Time cost=18.022\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:45 INFO 140489187047232] Epoch[4] Validation-accuracy=0.505208\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:38:58 INFO 140489187047232] Epoch[5] Batch [20]#011Speed: 53.206 samples/sec#011accuracy=0.492560\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:04 INFO 140489187047232] Epoch[5] Train-accuracy=0.497984\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:04 INFO 140489187047232] Epoch[5] Time cost=17.998\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:05 INFO 140489187047232] Epoch[5] Validation-accuracy=0.505208\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:18 INFO 140489187047232] Epoch[6] Batch [20]#011Speed: 53.033 samples/sec#011accuracy=0.526786\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:24 INFO 140489187047232] Epoch[6] Train-accuracy=0.510081\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:24 INFO 140489187047232] Epoch[6] Time cost=18.037\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:26 INFO 140489187047232] Epoch[6] Validation-accuracy=0.505208\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:38 INFO 140489187047232] Epoch[7] Batch [20]#011Speed: 53.081 samples/sec#011accuracy=0.491071\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:44 INFO 140489187047232] Epoch[7] Train-accuracy=0.497984\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:44 INFO 140489187047232] Epoch[7] Time cost=18.026\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:46 INFO 140489187047232] Epoch[7] Validation-accuracy=0.491071\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:39:58 INFO 140489187047232] Epoch[8] Batch [20]#011Speed: 52.818 samples/sec#011accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:04 INFO 140489187047232] Epoch[8] Train-accuracy=0.518145\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:04 INFO 140489187047232] Epoch[8] Time cost=18.121\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:06 INFO 140489187047232] Epoch[8] Validation-accuracy=0.494792\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:19 INFO 140489187047232] Epoch[9] Batch [20]#011Speed: 52.802 samples/sec#011accuracy=0.507440\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:25 INFO 140489187047232] Epoch[9] Train-accuracy=0.504032\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:25 INFO 140489187047232] Epoch[9] Time cost=18.076\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:26 INFO 140489187047232] Epoch[9] Validation-accuracy=0.500000\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:39 INFO 140489187047232] Epoch[10] Batch [20]#011Speed: 52.925 samples/sec#011accuracy=0.502976\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:45 INFO 140489187047232] Epoch[10] Train-accuracy=0.493952\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:45 INFO 140489187047232] Epoch[10] Time cost=18.057\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:46 INFO 140489187047232] Epoch[10] Validation-accuracy=0.484375\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:40:59 INFO 140489187047232] Epoch[11] Batch [20]#011Speed: 53.121 samples/sec#011accuracy=0.510417\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:05 INFO 140489187047232] Epoch[11] Train-accuracy=0.510081\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:05 INFO 140489187047232] Epoch[11] Time cost=18.048\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:07 INFO 140489187047232] Epoch[11] Validation-accuracy=0.504464\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:19 INFO 140489187047232] Epoch[12] Batch [20]#011Speed: 52.767 samples/sec#011accuracy=0.476190\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:25 INFO 140489187047232] Epoch[12] Train-accuracy=0.481855\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:25 INFO 140489187047232] Epoch[12] Time cost=18.073\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:27 INFO 140489187047232] Epoch[12] Validation-accuracy=0.484375\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:39 INFO 140489187047232] Epoch[13] Batch [20]#011Speed: 53.038 samples/sec#011accuracy=0.470238\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:45 INFO 140489187047232] Epoch[13] Train-accuracy=0.471774\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:45 INFO 140489187047232] Epoch[13] Time cost=18.014\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:47 INFO 140489187047232] Epoch[13] Validation-accuracy=0.489583\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:41:59 INFO 140489187047232] Epoch[14] Batch [20]#011Speed: 52.933 samples/sec#011accuracy=0.508929\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:05 INFO 140489187047232] Epoch[14] Train-accuracy=0.511089\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:05 INFO 140489187047232] Epoch[14] Time cost=18.106\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:07 INFO 140489187047232] Epoch[14] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:19 INFO 140489187047232] Epoch[15] Batch [20]#011Speed: 52.977 samples/sec#011accuracy=0.519345\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:25 INFO 140489187047232] Epoch[15] Train-accuracy=0.514113\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:25 INFO 140489187047232] Epoch[15] Time cost=18.042\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:27 INFO 140489187047232] Epoch[15] Validation-accuracy=0.477679\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:40 INFO 140489187047232] Epoch[16] Batch [20]#011Speed: 52.582 samples/sec#011accuracy=0.511905\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:46 INFO 140489187047232] Epoch[16] Train-accuracy=0.513105\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:46 INFO 140489187047232] Epoch[16] Time cost=18.139\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:42:47 INFO 140489187047232] Epoch[16] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:00 INFO 140489187047232] Epoch[17] Batch [20]#011Speed: 52.849 samples/sec#011accuracy=0.511905\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:06 INFO 140489187047232] Epoch[17] Train-accuracy=0.523185\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:06 INFO 140489187047232] Epoch[17] Time cost=18.158\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:08 INFO 140489187047232] Epoch[17] Validation-accuracy=0.505208\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:20 INFO 140489187047232] Epoch[18] Batch [20]#011Speed: 52.692 samples/sec#011accuracy=0.492560\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:26 INFO 140489187047232] Epoch[18] Train-accuracy=0.512097\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:26 INFO 140489187047232] Epoch[18] Time cost=18.118\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:28 INFO 140489187047232] Epoch[18] Validation-accuracy=0.505208\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:40 INFO 140489187047232] Epoch[19] Batch [20]#011Speed: 52.936 samples/sec#011accuracy=0.510417\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:46 INFO 140489187047232] Epoch[19] Train-accuracy=0.522177\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:46 INFO 140489187047232] Epoch[19] Time cost=18.056\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:43:48 INFO 140489187047232] Epoch[19] Validation-accuracy=0.491071\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:01 INFO 140489187047232] Epoch[20] Batch [20]#011Speed: 52.575 samples/sec#011accuracy=0.525298\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:07 INFO 140489187047232] Epoch[20] Train-accuracy=0.520161\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:07 INFO 140489187047232] Epoch[20] Time cost=18.228\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:08 INFO 140489187047232] Epoch[20] Validation-accuracy=0.484375\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:21 INFO 140489187047232] Epoch[21] Batch [20]#011Speed: 52.914 samples/sec#011accuracy=0.522321\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:27 INFO 140489187047232] Epoch[21] Train-accuracy=0.525202\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:27 INFO 140489187047232] Epoch[21] Time cost=18.057\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:28 INFO 140489187047232] Epoch[21] Validation-accuracy=0.479167\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:41 INFO 140489187047232] Epoch[22] Batch [20]#011Speed: 52.782 samples/sec#011accuracy=0.563988\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:47 INFO 140489187047232] Epoch[22] Train-accuracy=0.553427\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:47 INFO 140489187047232] Epoch[22] Time cost=18.099\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:49 INFO 140489187047232] Epoch[22] Validation-accuracy=0.520833\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:49 INFO 140489187047232] Storing the best model with validation accuracy: 0.520833\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:44:49 INFO 140489187047232] Saved checkpoint to \"/opt/ml/model/image-classification-0023.params\"\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:01 INFO 140489187047232] Epoch[23] Batch [20]#011Speed: 52.943 samples/sec#011accuracy=0.550595\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:07 INFO 140489187047232] Epoch[23] Train-accuracy=0.547379\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:07 INFO 140489187047232] Epoch[23] Time cost=18.109\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:09 INFO 140489187047232] Epoch[23] Validation-accuracy=0.424107\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:22 INFO 140489187047232] Epoch[24] Batch [20]#011Speed: 52.130 samples/sec#011accuracy=0.601190\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:28 INFO 140489187047232] Epoch[24] Train-accuracy=0.573589\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:28 INFO 140489187047232] Epoch[24] Time cost=18.231\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:29 INFO 140489187047232] Epoch[24] Validation-accuracy=0.515625\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:42 INFO 140489187047232] Epoch[25] Batch [20]#011Speed: 52.829 samples/sec#011accuracy=0.541667\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:48 INFO 140489187047232] Epoch[25] Train-accuracy=0.554435\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:48 INFO 140489187047232] Epoch[25] Time cost=18.075\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:45:49 INFO 140489187047232] Epoch[25] Validation-accuracy=0.494792\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:46:02 INFO 140489187047232] Epoch[26] Batch [20]#011Speed: 52.960 samples/sec#011accuracy=0.552083\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:46:08 INFO 140489187047232] Epoch[26] Train-accuracy=0.563508\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:46:08 INFO 140489187047232] Epoch[26] Time cost=18.127\u001b[0m\n",
      "\u001b[34m[09/13/2021 18:46:10 INFO 140489187047232] Epoch[26] Validation-accuracy=0.442708\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.fit(inputs=model_inputs, logs=\"All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awako-endpoint-2021-09-13-18-56-11\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"awako-endpoint-\"+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f\"{BASE_URL}/data_capture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "deployment = model.deploy(\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "serializer = IdentitySerializer(\"image/png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    session=sess,\n",
    "    serializer=serializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[0.500469446182251, 0.499530553817749]'\n"
     ]
    }
   ],
   "source": [
    "with open(\"./test/bicycle_s_001789.png\", \"rb\") as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = predictor.predict(data=payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_case():\n",
    "    # Setup s3 in boto3\n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    # Randomly pick from sfn or test folders in our bucket\n",
    "    objects = s3.Bucket(bucket).objects.filter(Prefix=\"test/bicycle\")\n",
    "    \n",
    "    # Grab any random object key from that folder!\n",
    "    obj = random.choice([x.key for x in objects])\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"image_data\": \"\",\n",
    "        \"s3_bucket\": bucket,\n",
    "        \"s3_key\": obj\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"image_data\": \"\", \"s3_bucket\": \"sagemaker-us-east-1-013268419919\", \"s3_key\": \"test/bicycle_s_001789.png\"}'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_test_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=\"data_capture\")\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in capture_files:\n",
    "    filename = file.split(\"/\")[-1]\n",
    "    s3_client.download_file(bucket, file, f\"captured_data/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the file names we downloaded\n",
    "file_handles = os.listdir(\"./captured_data\")\n",
    "\n",
    "# Dump all the data into an array\n",
    "json_data = []\n",
    "for jsonl in file_handles:\n",
    "    with jsonlines.open(f\"./captured_data/{jsonl}\") as f:\n",
    "        json_data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_getter(obj):\n",
    "    inferences = obj[\"captureData\"][\"endpointOutput\"][\"data\"]\n",
    "    timestamp = obj[\"eventMetadata\"][\"inferenceTime\"]\n",
    "    return json.loads(inferences), timestamp\n",
    "\n",
    "simple_getter(json_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for obj in json_data:\n",
    "    inference, timestamp = simple_getter(obj)\n",
    "    \n",
    "    y.append(max(inference))\n",
    "    x.append(timestamp)\n",
    "\n",
    "# Todo: here is an visualization example, take some time to build another visual that helps monitor the result\n",
    "# Plot the data\n",
    "plt.scatter(x, y, c=['r' if k<.94 else 'b' for k in y ])\n",
    "plt.axhline(y=0.94, color='g', linestyle='--')\n",
    "plt.ylim(bottom=.88)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel(\"Confidence\")\n",
    "plt.suptitle(\"Observed Recent Inferences\", size=14)\n",
    "plt.title(\"Pictured with confidence threshold for production use\", size=10)\n",
    "\n",
    "# Give it some pizzaz!\n",
    "plt.style.use(\"Solarize_Light2\")\n",
    "plt.gcf().autofmt_xdate()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
